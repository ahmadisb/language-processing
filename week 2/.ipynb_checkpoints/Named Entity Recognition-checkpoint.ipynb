{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the tokens and tags\n",
    "2. Identify the train, test, and validation datasets\n",
    "3. Print the data to see what types of stuffs we are dealing with\n",
    "4. Build a dictionary to prepare token -> tag relationship, or vice-versa\n",
    "5. Create placeholders to specify what data we are going to feed into the network during the execution time.\n",
    "        input_batch — sequences of words (the shape equals to [batch_size, sequence_len]);\n",
    "        ground_truth_tags — sequences of tags (the shape equals to [batch_size, sequence_len]);\n",
    "        lengths — lengths of not padded sequences (the shape equals to [batch_size]);\n",
    "        dropout_ph — dropout keep probability; this placeholder has a predefined value 1;\n",
    "        learning_rate_ph — learning rate; we need this placeholder because we want to change the value during training.\n",
    "6. Build the NN with following observations: \n",
    "\n",
    "        - Create embeddings matrix with tf.Variable. Specify its name (embeddings_matrix), type (tf.float32), and initialize with random values.\n",
    "        - Create forward and backward LSTM cells. TensorFlow provides a number of RNN cells ready for you. We suggest that you use BasicLSTMCell, but you can also experiment with other types, e.g. GRU cells. This blogpost could be interesting if you want to learn more about the differences.\n",
    "        - Wrap your cells with DropoutWrapper. Dropout is an important regularization technique for neural networks. Specify all keep probabilities using the dropout placeholder that we created before.\n",
    "        - After that, you can build the computation graph that transforms an input_batch:\n",
    "\n",
    "            * Look up embeddings for an input_batch in the prepared embedding_matrix.\n",
    "            * Pass the embeddings through Bidirectional Dynamic RNN with the specified forward & backward cells.\n",
    "            * Use the lengths placeholder here to avoid computations for padding tokens inside the RNN.\n",
    "            * Create a dense layer on top. Its output will be used directly in loss function.\n",
    "            \n",
    "        - Apply softmax to the last layer\n",
    "7. Use cross-entropy loss in the training data (applied in logits, not to the softmax probabilities)\n",
    "        - Mask the unnecessary <PAD> like dirty tags\n",
    "8. Optimize the loss by using Adam OPtimizer. Use clipping to remove exploding gradients\n",
    "9. Train the network\n",
    "10. Predict the tags\n",
    "11. Evaluate the model\n",
    "12. Run the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
