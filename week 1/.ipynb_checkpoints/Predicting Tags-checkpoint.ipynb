{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLabel Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step wise implementation of all process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\.  Import numpy, pandas, ntlk and other related dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Import the files, train - test and validation sets\n",
    "    - Initialize X_Train, X_Test, X_Val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Natural data is unstructured, so to remove the redundancy, we preprocess the text data\n",
    "    - convert everything to lower-case\n",
    "    - unnecessary symbols, like brackets, at-the-rate, commas,etc. needs to be replaced by spaces\n",
    "    - bad symbols (anything apart from alpha numeric and spaces) should be removed\n",
    "    - remove stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Text Pre-process on X_Train, X_Val and X_Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Show most popular tags and words from the train data (#EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Computers doesn't work on text. So transform all values into numberic vectors. There are two procedures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Bag of Words Model\n",
    "        \n",
    "    - Find N most popular words in train corpus.\n",
    "    - Then we need to numerate them, for example, like this: {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "    - For each title in the corpora create a zero vector with the dimension equals to N, like this: [0, 0, 0, 0]\n",
    "    - For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.\n",
    "            text: 'hi how are you'\n",
    "            corpus: 'hi you me are'\n",
    "            vector: [1, 1, 0, 1]\n",
    "            \n",
    "    - Use a dictionary size of 5000, and generate the most common words used in train data\n",
    "    - Use the model in all three available variants\n",
    "    - Show non-zero elements in the newly generated values of X-Train, X-Test, and X-Val (#EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. TF-IDF Model (penalize too frequent words)\n",
    "    \n",
    "    - Create a TF-IDF vectorizer with a proper parameters choice, and fit the result on train set (most important part: filter out most and rarely used words, and use 1-gram and bi-grams in the model)\n",
    "    - Transform the train, test, and val sets and return the result\n",
    "    - Check for the results (#EDA)\n",
    "    - Check if tags are available in the newly formed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Multi-Label Classifier (for multiple tags in a title)\n",
    "\n",
    "    - import dependencies and binarize y_train, and y_val\n",
    "    - transform the model on k numbers of tags from the train data\n",
    "    - Train the classifiers for different data transformations: bag-of-words and tf-idf.\n",
    "    - Now you can create predictions for the data. You will need two types of predictions: labels and scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Evaluation: \n",
    "    - checks which model are better\n",
    "    - checks whether or not to use the regularization techniques\n",
    "\n",
    "    - Accuracy (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "    - F1-score (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "    - Area under ROC-curve (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "    - Area under precision-recall curve (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC: \n",
    "- You might also want to plot some generalization of the ROC curve for the case of multi-label classification.\n",
    "- Provided function roc_auc can make it for you. The input parameters of this function are:\n",
    "\n",
    "        - true labels\n",
    "        - decision functions scores\n",
    "        - number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. After evaluating the model, experiment a bit with the classifier\n",
    "- Use L1 and L2 regularization techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
